{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Redes Neurais",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ucTSoyxY9GkC",
        "outputId": "515948d2-91f9-4e26-a244-3481754b5b5d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import keras\n",
        "keras.__version__"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.3.1'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VJBCbJ3i9QD0"
      },
      "source": [
        "# Redes Neurais\n",
        "\n",
        "04/2020\n",
        "\n",
        "# Resumo\n",
        "\n",
        "Redes Neurais são um modelo de aprendizado de máquina que imita a maneira biológica que os seres humanos aprendem. Uma rede neural consiste em um conjunto organizado de nêuronios que tomam pequenas decisões baseado nas entradas que recebe. A idéia é que, por mais que um neurônio sozinho seja insuficiente para resolver grandes problemas, vários neurônios unidos tomando pequenas decisões juntos são capazes de resolver corretamente grandes dificuldades do aprendizado de máquina.\n",
        "\n",
        "# Sumário\n",
        "\n",
        " <ol>\n",
        "<li>O neurônio</li>\n",
        "<li>A arquitetura de uma Rede Neural</li>\n",
        "<li>Como a Rede Neural aprende</li>\n",
        "<li>O problema: Reconhecendo dígitos escritos a mão</li>\n",
        "<li>Bibliografia</li>\n",
        "</ol>\n",
        "\n",
        "\n",
        "##1. O neurônio\n",
        "\n",
        "Um neurônio é a unidade de aprendizado de uma rede neural. Existem diversos tipos de neurônios, que normalmente se referem aos modelos de aprendizado de máquina mais comuns, temos neurônios que aprendem como o perceptron, a regressão logística, a regressão linear e o softmax.\n",
        "\n",
        "A unidade básica funciona entregando uma saída para um dado conjunto de entradas como a imagem a seguir:\n",
        "\n",
        "![alt text](http://neuralnetworksanddeeplearning.com/images/tikz9.png)\n",
        "\n",
        "A natureza das entradas e das saídas pode variar, por exemplo se o tipo de neurônio utilizado for um perceptron, ele funcionará dando um \"peso\" para cada entrada, calculando o produto interno $ <w^T,x>$, somando esse produto a um bias $b$ e entregando uma saída binária, $ 1 $ caso o resultado seja positivo, e $ 0 $ caso contrário.\n",
        "\n",
        "O modelo que dita a maneira de aprendizado de determinado neurônio é chamado função de ativação. Por exemplo se o neurônio se comporta da maneira descrita anteriormente, dizemos que a função que ativa o neurônio é o perceptron. Diferentes funções de ativação tem implicações diferentes no aprendizado de determinada rede neural, podendo ser um fator que influencia na velocidade de aprendizado ou na eficiência do modelo como um todo.\n",
        "\n",
        "###1.1 A função sigmóide\n",
        "\n",
        "Uma das funções de ativação mais utilizada é a função sigmoide, ela segue o mesmo princípio da regressão logística, ou seja, para um conjunto de entradas, entrega uma saída que é um número real entre $0$ e $1$. Básicamente sua saída é $\\sigma(<w^T,x> + \\space b) $ sendo $\\sigma (z) = \\frac {1}{1 \\space + \\space e^{-z}}$. A função sigmóide é mais amplamente utilizada que o perceptron porque a transição entre o $0$ e o $1$ é bem menos abrupta, fazendo que uma mudança pequena na entrada não altere tanto a saída.\n",
        "\n",
        "##2. A arquitetura de uma rede neural\n",
        "\n",
        "Agora que sabemos como é a menor unidade de aprendizado do modelo, podemos enxergar uma rede neural simplesmente como a união de vários neurônios, interligados entre si como a figura a seguir:\n",
        "\n",
        "![alt text](http://neuralnetworksanddeeplearning.com/images/tikz1.png)\n",
        "\n",
        "Uma rede neural é dividida em diversas camadas, contendo uma quantidade de neurônios em cada uma, em que as entradas e saídas de cada neurônio são ligadas nos neurônios das próximas camadas.\n",
        "\n",
        "Existem três tipos de camadas em uma rede neural:\n",
        "\n",
        "**Camada de entrada** (ou input layer) é a camada de neurônios em que os dados são colocados para serem enviados às próximas camadas. Esses neurônios não possuem entradas, nem pesos, nem bias, somente entregam como saída os dados do problema.\n",
        "\n",
        "**Camadas intermediárias** (ou hidden layers) são as camadas que estão entre as camadas de entrada e as de saída, se chamam \"hidden layers\" pois em alguns casos pode ser difícil entender o comportamento de seus pesos e bias ao longo do algoritmo. Uma rede neural pode ter nenhuma, uma ou mais camadas intermediárias.\n",
        "\n",
        "**Camadas de saída** (ou output layers) é a camada que entrega a saída do sistema, a quantidade de neurônios que ela terá dependerá do problema que a rede neural está resolvendo.\n",
        "\n",
        "\n",
        "##3. Como a Rede Neural aprende\n",
        "\n",
        "\n",
        "Assim como em alguns outros modelos, o aprendizado de uma rede neural se da pela atualização dos pesos e biases de cada neurônio segundo o algoritmo do gradiente descendente. A cada iteração, se atualiza os pesos e os biases na direção que levaria a minimizar o $E_{in}$ da amostra.\n",
        "\n",
        "As redes neurais se tornaram o principal modelo de aprendizado de máquina nos dias atuais. Isso se deve principalmente pela eficiência no cálculo do gradiente descendente do algoritmo mais utilizado em redes neurais, que é o  algoritmo de retropropagação (ou backpropagation), desde que ele surgiu, foi responsável por levar esse modelo a um novo grau de importância.\n",
        "\n",
        "O algoritmo de backpropagation se consiste no cálculo do gradiente descendente de uma maneira simples. Se baseando na regra da cadeia ele tornou possível dizer qual a direção em que cada peso $w$ e bias $b$ deve ser atualizado, levando-se em conta apenas sabendo a saída de seus neurônios. Ele primeiro calcula as saídas de cada neurônio baseado nas suas entradas atuais (indo da esquerda para a direita), quando se chega na saída final do modelo, é calculado um vetor $ \\delta^L $ em função da saída e da função de custo, e em seguida o algoritmo calcula da direita para a esquerda qual deve ser a atualização de cada $w$ e $b$.\n",
        "\n",
        "Não será de interesse desse notebook explicitar toda a matemática por trás do algoritmo de backpropagation, pois utilizaremos uma biblioteca de python que já calcula o gradiente descendente automáticamente, contudo ter um bom entendimento de como funciona o algoritmo é importante para fazer ajustes com o objetivo de melhorar o aprendizado das redes neurais.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kIQQcBTynHwE"
      },
      "source": [
        "##4. O problema: Reconhecendo digitos escritos a mão\n",
        "\n",
        "Para exemplificar o conteúdo deste notebook, resolveremos um problema que seria extremamente difícil de lidar com outros modelos, utilizando redes neurais. Este o problema se consiste em reconhecer dígitos escritos a mão utilizando aprendizado de máquina. \n",
        "\n",
        "Para isso utilizaremos o conjunto de dados MNIST que consiste em 60 mil imagens de treino e 10 mil imagens de teste para a verificação da qualidade do programa.\n",
        "Os dados são imagens do seguinte tipo:\n",
        "\n",
        "![alt text](http://neuralnetworksanddeeplearning.com/images/mnist_100_digits.png)\n",
        "\n",
        "Esse conjunto de dados já esta presente no keras, que é a biblioteca que utilizaremos nesse notebook para a implementação das redes neurais.\n",
        "O primeiro passo é importar o conjunto de dados e guardar em tensores.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FUx7yEExDvHL"
      },
      "source": [
        "from keras.datasets import mnist\n",
        "\n",
        "(imagens_treino, saidas_treino), (imagens_teste, saidas_teste) = mnist.load_data()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Scc1oVCXzPft"
      },
      "source": [
        "``` imagens_treino ``` e ``` saidas_treino ``` são arrays do numpy que serão responsáveis pelo aprendizado da rede neural. Já ```imagens_teste```, e ```saidas_teste``` são responsáveis pelo teste da eficiencia da rede.\n",
        "\n",
        "As imagens estão codificadas como arrays do numpy, e as saídas são um array simples de dígitos, indo de 0 a 9.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GIQQ0VLu1JVg"
      },
      "source": [
        "O próximo passo é montar nossa rede neural. Os hiper-parâmetros serão escolhidos segundo as heurísticas abordadas no \"Neural networks and Deep Learning\" do Michael Nielsen, aplicados ao problema.\n",
        "A rede neural construida será do tipo:\n",
        "\n",
        "![alt text](http://neuralnetworksanddeeplearning.com/images/tikz12.png)\n",
        "\n",
        "Com apenas uma camada intermediária com 100 neurônios (diferente da imagem), 784 neurônios na camada de entrada (já que as imagens MNIST possuem 784 pixels cada) e 10 neurônios na saída, segundo o modelo softmax. Cada neurônio da saída se enviará a probabilidade do dígito da entrada ser o dígito referido a cada neurônio. Sendo que a soma das probabilidades devem ser sempre 100% (softmax).\n",
        "A função de ativação dos neurônios da camada intermediária será a função sigmóide, e a função custo escolhida é a de entropia cruzada (cross-entropy). Além disso, para atenuar o overfitting, utilizaremos regularização do tipo L2 com $\\lambda = 0,001$.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aUxxeV3N14Ip"
      },
      "source": [
        "from keras import models\n",
        "from keras import layers\n",
        "from keras import regularizers\n",
        "\n",
        "\n",
        "network = models.Sequential()\n",
        "network.add(layers.Dense(100,\n",
        "                          activation='sigmoid', input_shape=(784,)))\n",
        "network.add(layers.Dense(10, activation='softmax'))\n",
        "\n",
        "network.compile(optimizer='rmsprop',\n",
        "                loss='categorical_crossentropy',\n",
        "                metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7e12gBU_6PKx"
      },
      "source": [
        "Agora, antes de treinar a Rede Neural criada, precisamos redimensionar os nossos arrays de entrada para o formato que a nossa Rede Neural espera. Além disso faremos uma normalização. Os pixels das imagens estão em escala de cinza de 8 bits, ou seja, cada imagem é um numpy array de dimensão 28x28 de números que vão de 0 a 255 (8 bits).\n",
        "\n",
        "Nossa intenção com as linhas a seguir é transformar cada imagem em um array de 784 valores de tipo float indo de 0 a 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VuBAa9tl8Ea9"
      },
      "source": [
        "imagens_treino = imagens_treino.reshape((60000, 28 * 28))\n",
        "imagens_treino = imagens_treino.astype('float32') / 255\n",
        "\n",
        "imagens_teste = imagens_teste.reshape((10000, 28 * 28))\n",
        "imagens_teste = imagens_teste.astype('float32') / 255\n",
        "\n",
        "\n",
        "\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "saidas_treino = to_categorical(saidas_treino)\n",
        "saidas_teste = to_categorical(saidas_teste)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E0uwI7zV-EL0"
      },
      "source": [
        "Agora treinaremos nossa Rede Neural com 30 épocas e um batch size de 10"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xkACOzYM-hRg",
        "outputId": "80a4da10-f0ff-46e0-9110-7a42ee02cec9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "network.fit(imagens_treino, saidas_treino, epochs=30, batch_size=10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "60000/60000 [==============================] - 12s 205us/step - loss: 0.3520 - accuracy: 0.9065\n",
            "Epoch 2/30\n",
            "60000/60000 [==============================] - 13s 215us/step - loss: 0.2198 - accuracy: 0.9424\n",
            "Epoch 3/30\n",
            "60000/60000 [==============================] - 11s 184us/step - loss: 0.1850 - accuracy: 0.9526\n",
            "Epoch 4/30\n",
            "60000/60000 [==============================] - 11s 184us/step - loss: 0.1648 - accuracy: 0.9582\n",
            "Epoch 5/30\n",
            "60000/60000 [==============================] - 11s 187us/step - loss: 0.1501 - accuracy: 0.9628\n",
            "Epoch 6/30\n",
            "60000/60000 [==============================] - 12s 193us/step - loss: 0.1409 - accuracy: 0.9657\n",
            "Epoch 7/30\n",
            "60000/60000 [==============================] - 11s 186us/step - loss: 0.1334 - accuracy: 0.9680\n",
            "Epoch 8/30\n",
            "60000/60000 [==============================] - 11s 182us/step - loss: 0.1281 - accuracy: 0.9693\n",
            "Epoch 9/30\n",
            "60000/60000 [==============================] - 11s 184us/step - loss: 0.1240 - accuracy: 0.9702\n",
            "Epoch 10/30\n",
            "60000/60000 [==============================] - 11s 185us/step - loss: 0.1210 - accuracy: 0.9710\n",
            "Epoch 11/30\n",
            "60000/60000 [==============================] - 11s 187us/step - loss: 0.1172 - accuracy: 0.9727\n",
            "Epoch 12/30\n",
            "60000/60000 [==============================] - 11s 184us/step - loss: 0.1157 - accuracy: 0.9726\n",
            "Epoch 13/30\n",
            "60000/60000 [==============================] - 11s 183us/step - loss: 0.1134 - accuracy: 0.9737\n",
            "Epoch 14/30\n",
            "60000/60000 [==============================] - 11s 184us/step - loss: 0.1115 - accuracy: 0.9737\n",
            "Epoch 15/30\n",
            "60000/60000 [==============================] - 12s 194us/step - loss: 0.1115 - accuracy: 0.9736\n",
            "Epoch 16/30\n",
            "60000/60000 [==============================] - 11s 186us/step - loss: 0.1081 - accuracy: 0.9742\n",
            "Epoch 17/30\n",
            "60000/60000 [==============================] - 11s 186us/step - loss: 0.1089 - accuracy: 0.9739\n",
            "Epoch 18/30\n",
            "60000/60000 [==============================] - 11s 185us/step - loss: 0.1065 - accuracy: 0.9748\n",
            "Epoch 19/30\n",
            "60000/60000 [==============================] - 11s 187us/step - loss: 0.1063 - accuracy: 0.9746\n",
            "Epoch 20/30\n",
            "60000/60000 [==============================] - 12s 197us/step - loss: 0.1058 - accuracy: 0.9751\n",
            "Epoch 21/30\n",
            "60000/60000 [==============================] - 11s 187us/step - loss: 0.1050 - accuracy: 0.9753\n",
            "Epoch 22/30\n",
            "60000/60000 [==============================] - 11s 185us/step - loss: 0.1024 - accuracy: 0.9761\n",
            "Epoch 23/30\n",
            "60000/60000 [==============================] - 11s 185us/step - loss: 0.1031 - accuracy: 0.9754\n",
            "Epoch 24/30\n",
            "60000/60000 [==============================] - 11s 185us/step - loss: 0.1010 - accuracy: 0.9765\n",
            "Epoch 25/30\n",
            "60000/60000 [==============================] - 12s 196us/step - loss: 0.1006 - accuracy: 0.9763\n",
            "Epoch 26/30\n",
            "60000/60000 [==============================] - 11s 189us/step - loss: 0.1021 - accuracy: 0.9761\n",
            "Epoch 27/30\n",
            "60000/60000 [==============================] - 11s 192us/step - loss: 0.1003 - accuracy: 0.9769\n",
            "Epoch 28/30\n",
            "60000/60000 [==============================] - 12s 205us/step - loss: 0.0996 - accuracy: 0.9766\n",
            "Epoch 29/30\n",
            "60000/60000 [==============================] - 12s 203us/step - loss: 0.1001 - accuracy: 0.9764\n",
            "Epoch 30/30\n",
            "60000/60000 [==============================] - 12s 206us/step - loss: 0.0980 - accuracy: 0.9776\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.callbacks.History at 0x7f5204f9d5c0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oA5HGOLJApkw"
      },
      "source": [
        "Agora para testar o nosso algoritmo fora do conjunto de treino, testaremos no conjunto de 10 mil imagens testes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "quBUWX5-Axkb",
        "outputId": "32bebcda-b46a-496d-f935-2e611eca704c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "test_loss, test_acc = network.evaluate(imagens_teste, saidas_teste)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10000/10000 [==============================] - 0s 31us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R0PWsKxCA3zW",
        "outputId": "3acd4210-534e-41ff-f916-e7ed0e4b1185",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "print(\"A acurácia do teste foi de: \", test_acc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "A acurácia do teste foi de:  0.9757000207901001\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EzEgmxToBVVE"
      },
      "source": [
        "Vimos então que com uma Rede Neural simples, com apenas uma camada intermediária e utilizando regularização, fomos capazes de fazer um algoritmo capaz de identificar dígitos escritos a mão, com índice de erro menor do que 3%. Isso mostra a capacidade das Redes Neurais de lidar com problemas complexos e entregar bons resultados."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b0dbRaBBC886"
      },
      "source": [
        "##5. Bibliografia \n",
        "\n",
        "[1] Learning From Data - Yaser S. Abu Mustafa\n",
        "\n",
        "[2] https://work.caltech.edu/lectures.html\n",
        "\n",
        "[3] http://cs231n.github.io/python-numpy-tutorial/\n",
        "\n",
        "[4] https://github.com/fchollet/deep-learning-with-python-notebooks\n",
        "\n",
        "[5] http://neuralnetworksanddeeplearning.com/index.html\n",
        "\n",
        "\n"
      ]
    }
  ]
}